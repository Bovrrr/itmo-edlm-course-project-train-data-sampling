{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a26e9fd-35ea-4d60-98d1-83e4d941d71d",
   "metadata": {},
   "source": [
    "# 1. Детерминизм + путь к проекту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81ad02ac-277b-4ab0-b9b2-c50b50f0a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic init done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- FULL DETERMINISM BLOCK ---\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"FLASH_ATTENTION_USE_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "# подключаем src проекта\n",
    "import sys\n",
    "sys.path.append(\"/home/onbaev.baurzhan/source/project/src\")\n",
    "\n",
    "print(\"Deterministic init done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9608bf-084a-46c8-8643-e041a19f7310",
   "metadata": {},
   "source": [
    "# 2. Загружаем SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e1c69f-91df-419a-9a06-4fb3c74b5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67349, 872)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "train_raw = ds[\"train\"]\n",
    "val_raw = ds[\"validation\"]\n",
    "\n",
    "len(train_raw), len(val_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c398f-3b2d-4b5b-bc7c-fe912805508f",
   "metadata": {},
   "source": [
    "# 3. Токенизация полного train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca0f773-8585-49f2-a4e5-3d765e3a5904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67349, 872)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    enc[\"label\"] = batch[\"label\"]\n",
    "    return enc\n",
    "\n",
    "train_tok_full = train_raw.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_raw.column_names\n",
    ")\n",
    "val_tok = val_raw.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=val_raw.column_names\n",
    ")\n",
    "\n",
    "len(train_tok_full), len(val_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b4cdd-2662-4fa9-8145-bce1e6c532a4",
   "metadata": {},
   "source": [
    "# 4. Формируем Random 10% для базовой модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31424861-88d2-4d4a-a9d4-449b5dbe9e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10 = train_raw.shuffle(seed=42).select(range(int(0.1 * len(train_raw))))\n",
    "train_tok_10 = train_10.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_raw.column_names\n",
    ")\n",
    "\n",
    "len(train_tok_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2363e4-a4e9-4489-a1ea-88c049c8c3d9",
   "metadata": {},
   "source": [
    "# 5. Обучаем базовую модель на Random 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f182c81-f187-4ab9-86c4-75cbd3be4b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:468.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "train_loss: 0.3929419008511785\n",
      "{'val_loss': 0.22405235016984598, 'accuracy': 0.9059633027522935, 'f1': 0.9076576576576577}\n",
      "\n",
      "Epoch 2\n",
      "train_loss: 0.16062389586950648\n",
      "{'val_loss': 0.22300875539492285, 'accuracy': 0.9208715596330275, 'f1': 0.9198606271777003}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.22300875539492285,\n",
       " 'accuracy': 0.9208715596330275,\n",
       " 'f1': 0.9198606271777003}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_utils import train_model\n",
    "\n",
    "base_model, base_metrics = train_model(\n",
    "    model_name=model_name,\n",
    "    train_dataset=train_tok_10,\n",
    "    val_dataset=val_tok,\n",
    "    epochs=2,        # 2 эпохи — быстро и достаточно для confidence/loss\n",
    "    lr=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "base_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9b6839-7c34-414b-b296-14d8396d2d57",
   "metadata": {},
   "source": [
    "# 6. DataLoader для скоринга (используем tokenizer.pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2985a86c-75dc-49da-a24a-c93846b60fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def make_loader_for_scoring(dataset, batch_size=64):\n",
    "    def collate_fn(batch):\n",
    "        enc = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [x[\"input_ids\"] for x in batch],\n",
    "                \"attention_mask\": [x[\"attention_mask\"] for x in batch],\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "scoring_loader = make_loader_for_scoring(train_tok_full, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852bdbd-9949-42ca-83de-89a791cbf012",
   "metadata": {},
   "source": [
    "# 7. Считаем p_gold (и/или loss) для каждого примера\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a888debe-a6c4-4fbf-b703-bf8391fdb24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "p_gold_list = []   # (idx, p_gold)\n",
    "\n",
    "base_model.eval()\n",
    "base_model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    global_idx = 0\n",
    "    for batch in scoring_loader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        logits = base_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            y = labels[i].item()\n",
    "            p = probs[i, y].item()  # вероятность истинного класса\n",
    "            p_gold_list.append((global_idx, p))\n",
    "            global_idx += 1\n",
    "\n",
    "len(p_gold_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0739a30-9399-4cc8-ba8f-26cd53de957b",
   "metadata": {},
   "source": [
    "# 8. Берём bottom-10% по p_gold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530b001c-2f40-4c25-84fb-5849c49754b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6734, [12086, 23459, 41765, 37621, 46803, 56987, 33677, 13418, 25401, 7250])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сортируем по уверенности ПО ВОЗРАСТАНИЮ\n",
    "p_gold_sorted = sorted(p_gold_list, key=lambda x: x[1])  # низкая уверенность → в начале\n",
    "\n",
    "M = int(0.1 * len(p_gold_sorted))\n",
    "low_conf_indices = [idx for idx, _ in p_gold_sorted[:M]]\n",
    "\n",
    "len(low_conf_indices), low_conf_indices[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6184eb-fcae-4109-a939-ffdb6674bd9b",
   "metadata": {},
   "source": [
    "# 9. Собираем low-confidence subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3349cbe-8d90-496b-8ff1-df7e451c7474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok_lowconf = train_tok_full.select(low_conf_indices)\n",
    "len(train_tok_lowconf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573dec9-40b3-478b-8235-83c7d91be676",
   "metadata": {},
   "source": [
    "# 10. Обучаем модель на Low-confidence 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d7ce5e-a313-4a32-a760-49cc63273e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "W1122 06:23:08.074000 140014858579008 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.accumulated_cache_size_limit (64)\n",
      "W1122 06:23:08.074000 140014858579008 torch/_dynamo/convert_frame.py:357]    function: 'compiled_mlp' (/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:528)\n",
      "W1122 06:23:08.074000 140014858579008 torch/_dynamo/convert_frame.py:357]    last reason: ___check_obj_id(L['self'], 140013880338576)                 \n",
      "W1122 06:23:08.074000 140014858579008 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1122 06:23:08.074000 140014858579008 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "train_loss: 0.6134442184208694\n",
      "{'val_loss': 1.3443306173597063, 'accuracy': 0.4518348623853211, 'f1': 0.6218354430379747}\n",
      "\n",
      "Epoch 2\n",
      "train_loss: 0.4624087393142601\n",
      "{'val_loss': 1.9485717288085393, 'accuracy': 0.20756880733944955, 'f1': 0.252972972972973}\n",
      "\n",
      "Epoch 3\n",
      "train_loss: 0.2629198776955288\n",
      "{'val_loss': 2.3707713527338847, 'accuracy': 0.2396788990825688, 'f1': 0.3480825958702065}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3707713527338847,\n",
       " 'accuracy': 0.2396788990825688,\n",
       " 'f1': 0.3480825958702065}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lowconf, metrics_lowconf = train_model(\n",
    "    model_name=model_name,\n",
    "    train_dataset=train_tok_lowconf,\n",
    "    val_dataset=val_tok,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "metrics_lowconf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uni-project)",
   "language": "python",
   "name": "uni-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
