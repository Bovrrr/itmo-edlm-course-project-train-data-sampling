{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec8d2d1-b90d-4b65-b2f2-9ca004d088d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic init done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- FULL DETERMINISM BLOCK ---\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"FLASH_ATTENTION_USE_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda_manual_seed_all = SEED\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "# добавить src проекта\n",
    "import sys\n",
    "sys.path.append(\"/home/onbaev.baurzhan/source/project/src\")\n",
    "\n",
    "print(\"Deterministic init done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4cea5f-8720-4a2c-9a63-6c7db1e631b7",
   "metadata": {},
   "source": [
    "# 2. Загрузка SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519197b6-74ae-45f4-bc69-5096a7ed1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(67349, 872)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\")\n",
    "train_raw = ds[\"train\"]\n",
    "val_raw = ds[\"validation\"]\n",
    "\n",
    "len(train_raw), len(val_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d8497-c31b-4003-a6ca-e40bce8c2896",
   "metadata": {},
   "source": [
    "# 3. Токенизация полного train (нужна для скоринга)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194e781d-a884-4132-bf62-7db556288bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67349, 872)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    enc[\"label\"] = batch[\"label\"]\n",
    "    return enc\n",
    "\n",
    "train_tok_full = train_raw.map(tokenize_batch, batched=True, remove_columns=train_raw.column_names)\n",
    "val_tok = val_raw.map(tokenize_batch, batched=True, remove_columns=val_raw.column_names)\n",
    "\n",
    "len(train_tok_full), len(val_tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda5ae5-693d-42c3-b451-48b6ed161194",
   "metadata": {},
   "source": [
    "# 4. обучим базовую модель на случайных 10% чтобы получить скоринг-оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3596e2b-a38d-485a-8ef5-e52eeb765ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:468.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "train_loss: 0.3929419008511785\n",
      "{'val_loss': 0.22405235016984598, 'accuracy': 0.9059633027522935, 'f1': 0.9076576576576577}\n",
      "\n",
      "Epoch 2\n",
      "train_loss: 0.16062389586950648\n",
      "{'val_loss': 0.22300875539492285, 'accuracy': 0.9208715596330275, 'f1': 0.9198606271777003}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.22300875539492285,\n",
       " 'accuracy': 0.9208715596330275,\n",
       " 'f1': 0.9198606271777003}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_10 = train_raw.shuffle(seed=42).select(range(int(0.1 * len(train_raw))))\n",
    "train_tok_10 = train_10.map(tokenize_batch, batched=True, remove_columns=train_10.column_names)\n",
    "\n",
    "from train_utils import train_model\n",
    "\n",
    "base_model, base_metrics = train_model(\n",
    "    model_name=model_name,\n",
    "    train_dataset=train_tok_10,\n",
    "    val_dataset=val_tok,\n",
    "    epochs=2,\n",
    "    lr=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "base_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3240379-4bd0-476f-ad8c-908158698185",
   "metadata": {},
   "source": [
    "# 5. получаем p_gold для каждого примера полного train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4645d529-29bf-4961-84a1-3b1fa3c395f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def make_loader_for_scoring(dataset, batch_size=64):\n",
    "    def collate_fn(batch):\n",
    "        enc = tokenizer.pad(\n",
    "            {\n",
    "                \"input_ids\": [x[\"input_ids\"] for x in batch],\n",
    "                \"attention_mask\": [x[\"attention_mask\"] for x in batch],\n",
    "            },\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "scoring_loader = make_loader_for_scoring(train_tok_full, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b0f6fc-6c5c-444b-a6c2-466b9bfaf826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_gold_list = []   # (global_idx, p_gold)\n",
    "\n",
    "base_model.eval()\n",
    "base_model.to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    global_idx = 0\n",
    "    for batch in scoring_loader:\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}  # перенос на GPU\n",
    "\n",
    "        logits = base_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "        ).logits\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            y = labels[i].item()\n",
    "            p_gold = probs[i, y].item()\n",
    "            p_gold_list.append((global_idx, p_gold))\n",
    "            global_idx += 1\n",
    "\n",
    "len(p_gold_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915e1bd-9278-4090-a154-aae27f57e9ce",
   "metadata": {},
   "source": [
    "# 6. Отбор top-10% по p_gold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4000a569-deda-4cde-96db-b25419d22e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6734, [4898, 45342, 9332, 11138, 20730, 47914, 6606, 56020, 35233, 31117])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сортируем по уверенности по убыванию\n",
    "p_gold_sorted = sorted(p_gold_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "M = int(0.1 * len(p_gold_sorted))\n",
    "high_conf_indices = [idx for idx, _ in p_gold_sorted[:M]]\n",
    "\n",
    "len(high_conf_indices), high_conf_indices[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a57461-b260-46f6-9a64-fe0ea5f341ac",
   "metadata": {},
   "source": [
    "# 7. Формируем high-confidence subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5559328-2377-4110-976e-2f10011357ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok_highconf = train_tok_full.select(high_conf_indices)\n",
    "len(train_tok_highconf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc34605-27e1-4719-80e8-c373597e5bbf",
   "metadata": {},
   "source": [
    "# 8. Обучение финальной модели на High-confidence 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057b7b96-3f93-4fa5-9f6a-fb0a0e861f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "W1122 06:43:48.091000 140380777315392 torch/_dynamo/convert_frame.py:357] torch._dynamo hit config.accumulated_cache_size_limit (64)\n",
      "W1122 06:43:48.091000 140380777315392 torch/_dynamo/convert_frame.py:357]    function: 'compiled_mlp' (/home/onbaev.baurzhan/source/project/.venv/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:528)\n",
      "W1122 06:43:48.091000 140380777315392 torch/_dynamo/convert_frame.py:357]    last reason: ___check_obj_id(L['self'], 140369908315408)                 \n",
      "W1122 06:43:48.091000 140380777315392 torch/_dynamo/convert_frame.py:357] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1122 06:43:48.091000 140380777315392 torch/_dynamo/convert_frame.py:357] To diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "train_loss: 0.09469464158386066\n",
      "{'val_loss': 0.4517337448362793, 'accuracy': 0.838302752293578, 'f1': 0.833530106257379}\n",
      "\n",
      "Epoch 2\n",
      "train_loss: 0.0014963871627682996\n",
      "{'val_loss': 0.728474582146321, 'accuracy': 0.8165137614678899, 'f1': 0.7927461139896373}\n",
      "\n",
      "Epoch 3\n",
      "train_loss: 5.144302978323316e-05\n",
      "{'val_loss': 0.7771362116826432, 'accuracy': 0.819954128440367, 'f1': 0.7974193548387096}\n",
      "\n",
      "Epoch 4\n",
      "train_loss: 2.7135464264040203e-05\n",
      "{'val_loss': 0.8158349139349801, 'accuracy': 0.8222477064220184, 'f1': 0.8010269576379975}\n",
      "\n",
      "Epoch 5\n",
      "train_loss: 1.7762077936391567e-05\n",
      "{'val_loss': 0.8417911894087281, 'accuracy': 0.8245412844036697, 'f1': 0.8040973111395646}\n",
      "\n",
      "Epoch 6\n",
      "train_loss: 1.2602813183578451e-05\n",
      "{'val_loss': 0.86384023912251, 'accuracy': 0.8268348623853211, 'f1': 0.8071519795657727}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.86384023912251,\n",
       " 'accuracy': 0.8268348623853211,\n",
       " 'f1': 0.8071519795657727}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_highconf, metrics_highconf = train_model(\n",
    "    model_name=model_name,\n",
    "    train_dataset=train_tok_highconf,\n",
    "    val_dataset=val_tok,\n",
    "    epochs=6,\n",
    "    lr=2e-5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "metrics_highconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc7944-52be-4f52-8d7c-d3e80966b3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34626e6-ebc3-4b6f-9cdb-daf6394b18c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ffd7b-eb16-4eb4-b163-bdd11088d7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35785f52-daf6-4766-8477-c9afc5428256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685287bc-ffe0-4cc1-9475-edba78a81d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c593d-605e-4cd5-9121-1998e0abc79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d7d49-b27d-42f6-a46f-2237a95a03cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uni-project)",
   "language": "python",
   "name": "uni-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
